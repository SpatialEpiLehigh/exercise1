---
title: "Exercise 1: Geocoding Exercise: Philadelphia Homicides 2025"
author: "Eric Delmelle"
date: "Spring 2026"
format: 
  html:
    toc: true
    toc-depth: 3
    code-fold: show
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = TRUE, warning = FALSE)
```

# Introduction

## Learning Objectives

By the end of this exercise, you will be able to:

1. Geocode a set of addresses using the Census Bureau geocoder
2. Calculate and interpret geocoding match rates
3. Identify and analyse failed geocodes
4. Create interactive maps to visualise spatial and temporal patterns
5. Understand how geocoding quality affects downstream spatial analysis
6. Apply geographic masking (geomasking) to protect privacy while preserving spatial patterns
7. Apply the full geocoding-to-visualisation workflow independently on a new dataset of your choosing

## Background

This exercise uses criminal homicides that occurred in Philadelphia during 2025. The dataset includes incident details and address information, but **coordinates have been removed**. You must geocode these addresses and analyze the spatial and temporal patterns.

---

# Setup
```{r load-packages}
library(tidyverse)
library(tidygeocoder)
library(sf)
library(leaflet)
library(htmlwidgets)
library(lubridate)
library(knitr)
library(tibble)

# Set theme for plots
theme_set(theme_minimal(base_size = 12))
```

---

# Part 1: Data Exploration

## Task 1.1: Load the Data
```{r load-data}
# Load the homicide dataset and parse the date column once
homicides <- read_csv("philadelphia_homicides_2025.csv", show_col_types = FALSE) %>%
  mutate(dispatch_date = mdy(dispatch_date))

# Examine the structure
glimpse(homicides)

# Display first 10 rows
kable(head(homicides, 10), caption = "First 10 homicide incidents")
```

**Question:** How many homicide incidents are in the dataset?

**ANSWER:** There are **`r nrow(homicides)`** homicide incidents in the 2025 Philadelphia dataset.

## Explore the dataset

### Temporal Patterns: By Hour of Day


```{r hourly-viz, fig.width=10, fig.height=6}
# Count incidents by hour (0-23 format)
hourly_counts <- homicides %>%
  count(hour, name = "homicides") %>%
  arrange(hour)

# Create bar plot
ggplot(hourly_counts, aes(x = hour, y = homicides)) +
  geom_col(fill = "steelblue", alpha = 0.7) +
  geom_text(aes(label = homicides), vjust = -0.5, size = 2.5) +
  scale_x_continuous(breaks = seq(0, 23, 2)) +
  labs(
    title = "Philadelphia Homicides by Hour of Day (2025)",
    subtitle = paste("Total homicides:", sum(hourly_counts$homicides)),
    x = "Hour (0 = midnight, 23 = 11pm)",
    y = "Number of Homicides"
  ) +
  theme_minimal(base_size = 12) +
  ylim(0, max(hourly_counts$homicides) * 1.15)

# Group into time periods for summary
hourly_summary <- hourly_counts %>%
  mutate(
    time_period = case_when(
      hour >= 0 & hour < 6 ~ "Late Night (12am-6am)",
      hour >= 6 & hour < 12 ~ "Morning (6am-12pm)",
      hour >= 12 & hour < 18 ~ "Afternoon (12pm-6pm)",
      hour >= 18 & hour < 24 ~ "Evening (6pm-12am)"
    )
  ) %>%
  group_by(time_period) %>%
  summarize(homicides = sum(homicides)) %>%
  mutate(percentage = round(homicides / sum(homicides) * 100, 1))

kable(hourly_summary,
      caption = "Homicides by Time Period",
      col.names = c("Time Period", "Homicides", "Percentage"))
```



---

# Part 2: Geocoding

## Task 2.1: Geocode with Census Bureau
```{r geocode-census}

homicides_census <- homicides %>%
  mutate(
    clean_address = location_block %>%
      str_replace(" BLOCK ", " ") %>%
      str_remove_all('"') %>%                    # Remove quotes
      str_replace(" AV$", " AVE") %>%            # Fix truncated AV
      str_replace(" A$", " AVE") %>%             # Fix truncated A
      str_replace(" ST$", " STREET") %>%         # Expand ST (optional)
      str_replace("WAKELNG", "WAKELING") %>%     # Fix known misspelling
      str_replace("MUHFIELD", "MUHLFIELD"),      # Fix known misspelling
    full_address = paste(clean_address, "Philadelphia, PA")
  ) %>%
  geocode(
    address = full_address,
    method = "census",
    lat = latitude,
    long = longitude,
    verbose = TRUE
  )

# Check what was added
cat("Columns added by geocoding:\n")
new_cols <- setdiff(colnames(homicides_census), colnames(homicides))
cat(paste(new_cols, collapse = ", "), "\n")

# Quick success rate check
cat(
  "\nGeocoding success rate:",
  mean(!is.na(homicides_census$latitude)) * 100,
  "%\n"
)
```


**Question:** What columns were added by the geocoding process?

**ANSWER:** The `geocode()` function adds two columns: **`latitude`** and **`longitude`**, named by the `lat =` and `long =` arguments in the call above.

---

## Task 2.2: Calculate Match Rate
```{r match-rate-census}
# Calculate match rate
match_rate_census <- mean(!is.na(homicides_census$latitude)) * 100
geocoded_count <- sum(!is.na(homicides_census$latitude))
failed_count <- sum(is.na(homicides_census$latitude))

# Summary statistics
cat("=== CENSUS GEOCODER RESULTS ===\n")
cat("Total addresses:", nrow(homicides_census), "\n")
cat("Successfully geocoded:", geocoded_count, "\n")
cat("Failed to geocode:", failed_count, "\n")
cat("Match rate:", round(match_rate_census, 1), "%\n\n")
```

**Question:** What is your geocoding match rate with the Census Bureau geocoder?

**ANSWER:** The match rate is **`r round(match_rate_census, 1)`%**. 

**NOTE:** A typical match rate for US addresses ranges from 85-98%. Block-level addresses (like "2800 BLOCK PROSPECT ST") may have lower match rates than exact addresses because:
- The geocoder must interpolate along the street segment
- Some street segments may be missing from TIGER files
- Block-level addresses are inherently less precise

---

## Task 2.3: Examine Failed Geocodes
```{r failed-geocodes-census}
failed_census <- homicides_census %>%
  filter(is.na(latitude)) %>%
  select(objectid, dispatch_date, location_block, dc_key)

kable(
  head(failed_census, 10),
  caption = "Failed Geocodes (first 10)",
  col.names = c("Object ID", "Date", "Address", "DC Key")
)

write_csv(failed_census, "failed_geocodes_census.csv")
cat("\nSaved", nrow(failed_census), "failed addresses to: failed_geocodes_census.csv\n")

```

**Question:** List 3-5 examples of addresses that failed to geocode. What patterns do you notice?

**ANSWER:** Examples of failed addresses:
```{r show-failed-examples}
if(nrow(failed_census) > 0) {
  failed_census %>% 
    head(5) %>% 
    pull(location_block) %>%
    cat(sep = "\n")
} else {
  cat("All addresses geocoded successfully!")
}
```

**NOTE:** Common patterns in failed geocodes include:
- Incomplete street names (e.g., "N" instead of "NORTH")
- Non-standard abbreviations (e.g., "BLVD" vs "BOULEVARD")
- Missing street segments in the TIGER reference database
- Intersections rather than specific addresses (e.g., "15TH ST & WALNUT ST")
- Misspellings or unusual street names

---

# Part 3: Spatial Analysis and Visualization

## Task 3.1: Create Spatial Objects
```{r create-sf}
# Use Census geocoding results; drop cases that failed
homicides_sf <- homicides_census %>%
  filter(!is.na(latitude) & !is.na(longitude)) %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326)  # WGS84

cat("Successfully created spatial object with", nrow(homicides_sf), "homicides\n")
```

**Question:** How many homicides were successfully geocoded and included in your spatial object?

**ANSWER:** **`r nrow(homicides_sf)`** homicides were successfully geocoded and converted to a spatial object.

**NOTE:** This represents `r round(nrow(homicides_sf)/nrow(homicides)*100, 1)`% of the original `r nrow(homicides)` homicides. The missing `r nrow(homicides) - nrow(homicides_sf)` cases (`r round((nrow(homicides) - nrow(homicides_sf))/nrow(homicides)*100, 1)`%) could not be mapped due to geocoding failure. If these failures are spatially clustered, our analysis will be biased.

---

## Task 3.2: Static Map
```{r static-map, fig.width=10, fig.height=8}
# Set up plot parameters
par(mar = c(4, 4, 3, 1))

# Get bounding box
bbox <- st_bbox(homicides_sf)

# Create plot
plot(st_geometry(homicides_sf),
     pch = 19,
     col = adjustcolor("red", alpha.f = 0.6),
     cex = 0.8,
     main = "Philadelphia Homicides 2025",
     xlab = "Longitude",
     ylab = "Latitude",
     axes = TRUE)

# Add grid
grid()

# Add count
text(bbox["xmin"], bbox["ymax"], 
     paste("n =", nrow(homicides_sf)), 
     pos = 4, cex = 0.9)
```

**Question:** Based on the static map, do homicides appear to be randomly distributed across Philadelphia or are there apparent clusters?

**ANSWER:** Homicides appear to be **spatially clustered** rather than randomly distributed. Visual inspection suggests:
- Higher concentration in certain areas of the city
- Some areas with very few or no homicides
- Possible clustering in what appears to be central/north Philadelphia

**NOTE:** This visual assessment should be confirmed with formal statistical tests (e.g., Moran\'s I, Ripley\'s K-function, or spatial scan statistics). The apparent clustering could reflect:
1. True spatial clustering of homicide risk factors
2. Population density patterns (more people = more potential victims)
3. Spatially varying socioeconomic conditions
4. Geographic variation in policing and reporting

---

## Task 3.3: Interactive Map - Basic
```{r leaflet-basic, fig.width=10, fig.height=8}
# Create basic interactive map
map_basic <- homicides_sf %>%
  leaflet() %>%
  addProviderTiles(providers$CartoDB.Positron) %>%
  addCircleMarkers(
    popup = ~paste0(
      "<strong>Homicide</strong><br>",
      "Date: ", dispatch_date, "<br>",
      "Time: ", hour, ":00<br>",
      "Location: ", location_block, "<br>",
      "District: ", dc_dist, "<br>",
      "Object ID: ", objectid
    ),
    radius = 4,
    color = "darkred",
    fillColor = "red",
    fillOpacity = 0.7,
    weight = 2,
    label = ~paste("District", dc_dist)
  ) %>%
  addScaleBar(position = "bottomleft") %>%
  addMiniMap(
    toggleDisplay = TRUE,
    minimized = TRUE,
    position = "bottomright"
  ) %>%
  addControl(
    html = paste0("<div style=\'background: white; padding: 5px; border-radius: 5px;\'><strong>Philadelphia Homicides 2025</strong><br>n = ", nrow(homicides_sf), "</div>"),
    position = "topright"
  )

# Display the map
map_basic

# Save the map
saveWidget(map_basic, "philadelphia_homicides_basic.html", selfcontained = TRUE)
cat("\nMap saved to: philadelphia_homicides_basic.html\n")
```

**Question:** Explore the interactive map. Which areas of Philadelphia have the highest concentration of homicides?

**ANSWER:** Based on interactive exploration, the highest concentrations appear in:
- **North Philadelphia** (Districts 22, 24, 25, 35)
- **West Philadelphia** (Districts 16, 18, 19)
- Some concentration in **Southwest Philadelphia** (District 12)

**NOTE:** Students should zoom in and click on individual points to see patterns. Encourage them to:
- Look for clustering within districts
- Note areas with very few homicides (likely wealthy/suburban areas)
- Consider how population density might explain patterns
- Think about historical patterns of segregation and disinvestment

---

## Task 3.4: Enhanced Map - Time of Day
```{r leaflet-time, fig.width=10, fig.height=8}
# Categorize by time of day
homicides_sf <- homicides_sf %>%
  mutate(
    time_period = case_when(
      hour >= 0 & hour < 6 ~ "Late Night (12am-6am)",
      hour >= 6 & hour < 12 ~ "Morning (6am-12pm)",
      hour >= 12 & hour < 18 ~ "Afternoon (12pm-6pm)",
      hour >= 18 & hour < 24 ~ "Evening (6pm-12am)"
    ),
    time_period = factor(time_period, levels = c(
      "Morning (6am-12pm)",
      "Afternoon (12pm-6pm)",
      "Evening (6pm-12am)",
      "Late Night (12am-6am)"
    ))
  )

# Create color palette
time_colors <- colorFactor(
  palette = c("#FFC107", "#FF9800", "#F44336", "#880E4F"),
  domain = homicides_sf$time_period
)

# Create map with time periods
map_time <- homicides_sf %>%
  leaflet() %>%
  addProviderTiles(providers$CartoDB.Positron) %>%
  addCircleMarkers(
    popup = ~paste0(
      "<strong>Homicide</strong><br>",
      "Date: ", dispatch_date, "<br>",
      "Time: ", hour, ":00 (", time_period, ")<br>",
      "Location: ", location_block, "<br>",
      "District: ", dc_dist
    ),
    radius = 4,
    color = ~time_colors(time_period),
    fillColor = ~time_colors(time_period),
    fillOpacity = 0.8,
    weight = 2,
    group = ~time_period
  ) %>%
  addLayersControl(
    overlayGroups = levels(homicides_sf$time_period),
    options = layersControlOptions(collapsed = FALSE)
  ) %>%
  addLegend(
    position = "topright",
    pal = time_colors,
    values = ~time_period,
    title = "Time of Day",
    opacity = 1
  ) %>%
  addScaleBar(position = "bottomleft")

# Display map
map_time

# Save
saveWidget(map_time, "philadelphia_homicides_timeofday.html", selfcontained = TRUE)
cat("\nMap saved to: philadelphia_homicides_timeofday.html\n")
```

**Question:** Toggle the different time periods on/off. Do spatial patterns vary by time of day?

**ANSWER:** Yes, spatial patterns show temporal variation:
- **Late night homicides** (12am-6am) are the most common and appear concentrated in specific areas
- **Morning homicides** (6am-12pm) are least common and more dispersed
- **Evening homicides** (6pm-12am) show moderate clustering
- Some districts show more variation by time than others

**NOTE:** This spatiotemporal pattern suggests:
1. Different causal mechanisms may be at work at different times
2. Nighttime economy (bars, clubs) may concentrate risk
3. Targeted interventions could be time-specific
4. Resource allocation (police patrols) should consider temporal patterns

---

## Task 3.5: Map by Police District
```{r leaflet-districts, fig.width=10, fig.height=8}
# Count homicides by district
district_summary <- homicides_sf %>%
  st_drop_geometry() %>%
  count(dc_dist, name = "homicides") %>%
  arrange(desc(homicides))

kable(district_summary,
      caption = "Homicides by Police District (Geocoded Cases Only)",
      col.names = c("District", "Homicides"))

# Create district-colored map
district_colors <- colorFactor(
  palette = "Set3",
  domain = homicides_sf$dc_dist
)

map_districts <- homicides_sf %>%
  leaflet() %>%
  addProviderTiles(providers$CartoDB.Positron) %>%
  addCircleMarkers(
    popup = ~paste0(
      "<strong>District ", dc_dist, "</strong><br>",
      "Date: ", dispatch_date, "<br>",
      "Time: ", hour, ":00<br>",
      "Location: ", location_block
    ),
    radius = 4,
    color = ~district_colors(dc_dist),
    fillColor = ~district_colors(dc_dist),
    fillOpacity = 0.7,
    weight = 2,
    group = ~paste("District", dc_dist)
  ) %>%
  addLayersControl(
    overlayGroups = unique(paste("District", sort(homicides_sf$dc_dist))),
    options = layersControlOptions(collapsed = FALSE)
  ) %>%
  addLegend(
    position = "topright",
    pal = district_colors,
    values = ~dc_dist,
    title = "Police District",
    opacity = 1
  )

map_districts
```

**Question:** Which 3 police districts have the highest number of homicides?

**ANSWER:** Based on the district summary table:
1. District **`r district_summary$dc_dist[1]`**: **`r district_summary$homicides[1]`** homicides
2. District **`r district_summary$dc_dist[2]`**: **`r district_summary$homicides[2]`** homicides
3. District **`r district_summary$dc_dist[3]`**: **`r district_summary$homicides[3]`** homicides

**NOTE:** These top districts account for `r round(sum(district_summary$homicides[1:3])/sum(district_summary$homicides)*100, 1)`% of all geocoded homicides. This concentration suggests resource allocation should be targeted to high-burden areas.

---

# Part 4: Temporal Analysis

## Task 4.1: Homicides by Time of Day
```{r time-analysis, fig.width=10, fig.height=6}
# Count by time period
time_summary <- homicides_sf %>%
  st_drop_geometry() %>%
  count(time_period) %>%
  mutate(
    percentage = n / sum(n) * 100
  )

kable(time_summary,
      caption = "Homicides by Time of Day",
      col.names = c("Time Period", "Count", "Percentage"),
      digits = c(0, 0, 1))

# Visualize
ggplot(time_summary, aes(x = time_period, y = n, fill = time_period)) +
  geom_col(show.legend = FALSE) +
  geom_text(aes(label = paste0(n, "\n(", round(percentage, 1), "%)")), 
            vjust = -0.5, size = 4) +
  scale_fill_manual(values = c("#FFC107", "#FF9800", "#F44336", "#880E4F")) +
  labs(
    title = "Philadelphia Homicides by Time of Day (2025)",
    subtitle = paste("Total geocoded homicides:", sum(time_summary$n)),
    x = "Time Period",
    y = "Number of Homicides"
  ) +
  theme_minimal(base_size = 13) +
  theme(axis.text.x = element_text(angle = 20, hjust = 1)) +
  ylim(0, max(time_summary$n) * 1.15)
```

**Question:** During which time period do most homicides occur?

**ANSWER:** Most homicides occur during **`r time_summary$time_period[which.max(time_summary$n)]`**, accounting for **`r round(time_summary$percentage[which.max(time_summary$n)], 1)`%** of all homicides (n = `r time_summary$n[which.max(time_summary$n)]`).

**NOTE:** The concentration of homicides in late night/evening hours suggests:
- Role of nighttime economy (bars, social gatherings)
- Reduced guardianship (fewer witnesses, police)
- Alcohol involvement likely
- Different intervention strategies needed for different times


---

# Part 5: Critical Reflection

## Question: Geocoding Quality Assessment

**Reflect on the geocoding process. What percentage of addresses could NOT be geocoded? How might these missing locations affect your spatial analysis?**

**ANSWER:**
```{r geocoding-summary}
total_incidents <- nrow(homicides)
successfully_geocoded <- nrow(homicides_sf)
failed_to_geocode <- total_incidents - successfully_geocoded
failure_rate <- (failed_to_geocode / total_incidents) * 100

cat("Geocoding Summary:\n")
cat("Total incidents:", total_incidents, "\n")
cat("Successfully geocoded:", successfully_geocoded, "(", 
    round(100 - failure_rate, 1), "%)\n")
cat("Failed to geocode:", failed_to_geocode, "(", 
    round(failure_rate, 1), "%)\n\n")
```

**Impact on spatial analysis:**

1. **Reduced sample size**: We lost `r failed_to_geocode` cases (`r round(failure_rate, 1)`%), reducing statistical power to detect spatial patterns.

2. **Potential spatial bias**: 
   - If geocoding failures are randomly distributed → estimates remain unbiased but less precise
   - If failures cluster spatially → systematic bias in hot spot detection
   - Rural or newly developed areas often have lower geocoding rates

3. **District-level effects**:
   - Some districts may be underrepresented if geocoding failures concentrated there
   - Rate calculations (homicides per capita) will be underestimated in affected areas

4. **Cannot assess patterns in failed areas**: 
   - Hot spots may exist in areas with many failures but go undetected
   - Resource allocation decisions could be biased against these areas

**NOTE:** This is why the Zimmerman et al. (2008) paper is so important - it showed that geocoding failures ARE spatially clustered, and when disease is associated with geocoding failure (e.g., rural disease + rural geocoding failure), statistical power drops by 50-70%!

---

## Question: Positional Uncertainty from Block-Level Addresses

**The addresses in this dataset are recorded at the block level (e.g., "2800 BLOCK PROSPECT ST").  The Census geocoder places each address somewhere along the corresponding street segment, but the true incident location could be anywhere on that block — typically a 50–200 m range.  Discuss how this inherent positional uncertainty could affect each of the following analyses:**

**a) Identifying homicide hot spots?**

**ANSWER:** A 50–200 m positional uncertainty can:

- Blur the true boundaries of spatial clusters
- Move cases in or out of a detected cluster depending on where along the block they are placed
- Reduce statistical power, making it harder to detect real clusters
- Create spurious clusters if the geocoder systematically places block-level addresses at the same interpolation point
- Be particularly problematic for small-area clusters (radius < 500 m)

**b) Calculating distance to the nearest police station?**

**ANSWER:**

- The positional uncertainty translates directly into roughly the same magnitude of error in any distance calculation
- An incident near the boundary of a 500 m buffer around a station could be misclassified as inside or outside that buffer
- This affects accessibility analyses and response-time estimates
- Areas with many block-level addresses near station boundaries are most vulnerable to this bias

**c) Determining whether a homicide occurred within a specific neighbourhood?**

**ANSWER:**

- Census tracts are typically 1–4 blocks across; a 50–200 m shift can easily cross a tract boundary
- Incidents near neighbourhood borders may be assigned to the wrong administrative unit
- This biases rate calculations for the affected neighbourhoods
- Small neighbourhoods and narrow boundary zones are at highest risk of misclassification

**NOTE:** Connect this to the Olson et al. (2006) paper — they showed that aggregating to census tract centroids (which introduces comparable positional error) reduced cluster-detection power by 20–30%. Also relevant: Jacquez (2012) documents mean positional errors of 58–614 m across different geocoding settings.

---

## Question: Spatial Patterns

**Based on your maps and analyses, describe the spatial distribution of homicides in Philadelphia. Are they concentrated in certain areas or evenly distributed?**

**ANSWER:**

Homicides in Philadelphia 2025 show **strong spatial concentration**:

1. **Geographic concentration**: 

   - Top 3 districts account for `r if(nrow(district_summary)>=3) round(sum(district_summary$homicides[1:3])/sum(district_summary$homicides)*100,1) else "~40"`% of all homicides
   - Clear visual clustering in North and West Philadelphia
   - Some areas have virtually no homicides

2. **Spatial heterogeneity**:

   - Not all areas within high-burden districts equally affected
   - Micro-level clustering within districts visible on interactive maps
   - Some street segments/blocks have multiple incidents

3. **Possible explanations** (would require additional data to confirm):

   - Correlation with socioeconomic disadvantage
   - Historical patterns of segregation and disinvestment
   - Population density variations
   - Drug market locations
   - Gang territory boundaries

4. **Statistical confirmation needed**:

   - Visual patterns should be tested with Moran\'s I or spatial scan statistics
   - Need to account for population at risk (denominators)
   - Consider other spatial risk factors

**NOTE:** This provides teachable moment about:

- Ecological fallacy (area-level patterns ≠ individual risk)
- Need for appropriate denominators (incidents per capita, not just counts)
- Importance of theory in interpreting spatial patterns
- Ethics of mapping stigmatizing outcomes

---

## Question: Temporal Patterns

**Describe any temporal patterns you observed (time of day, day of week, monthly trends). How might these patterns inform public safety strategies?**

**ANSWER:**

**Key temporal patterns:**

1. **Time of day**: `r round(max(time_summary$percentage), 1)`% of homicides occur during `r time_summary$time_period[which.max(time_summary$percentage)]`
   - Suggests role of nighttime economy, alcohol, reduced guardianship

2. **Day of week**: 
```{r dow-weekend-effect}
if(exists("weekend_avg") && weekend_avg > weekday_avg) {
  cat("   - Weekend elevation (", round(weekend_avg, 1), " vs ", 
      round(weekday_avg, 1), " on weekdays)\n", sep="")
  cat("   - Peak on ", homicides_dow$day_of_week[which.max(homicides_dow$n)], 
      " (", max(homicides_dow$n), " homicides)\n", sep="")
}
```

3. **Seasonal pattern**:
```{r seasonal-effect}
if(exists("summer_avg") && summer_avg > winter_avg) {
  cat("   - Summer months show ", round(summer_avg/winter_avg, 1), 
      "× more homicides than winter\n", sep="")
}
```

**Implications for public safety strategies:**

1. **Temporal resource allocation**:

   - Concentrate patrols during high-risk hours (`r time_summary$time_period[which.max(time_summary$percentage)]`)
   - Increase weekend staffing
   - Seasonal surge capacity for summer months

2. **Place-based interventions**:

   - Target nightlife districts during peak hours
   - Hot spot policing in specific locations at specific times
   - "Pulling levers" strategies focused on high-risk times

3. **Prevention programs**:

   - Violence interruption programs active during high-risk hours
   - Community events to provide alternative activities
   - Cooling centers in summer (if heat-violence link confirmed)

4. **Data-driven scheduling**:

   - Shift schedules based on temporal patterns
   - Predictive policing (with appropriate safeguards)
   - Real-time resource deployment

**NOTE:** Emphasize:

- Temporal patterns provide actionable intelligence
- But must be combined with spatial patterns (spatiotemporal analysis)
- Ethical concerns about over-policing
- Need for root cause approaches, not just enforcement



# Part 6: Geographic Masking for Privacy Protection

## Introduction to Geomasking

We have discussed the ethical considerations of working with sensitive location data. Even though our Philadelphia homicide dataset uses block-level addresses rather than exact locations, the combination of date, time, and approximate location could potentially allow re-identification of victims through news reports or public records.

**Geographic masking** (or geomasking) is a family of techniques that deliberately introduce positional error to protect privacy while preserving spatial patterns for analysis. The fundamental trade-off is:

- **More displacement** → Better privacy protection → Less analytical precision
- **Less displacement** → Weaker privacy protection → Better analytical precision

The most straightforward approach is **random perturbation**, where each point is displaced by a random distance in a random direction.

## The Random Perturbation Method

Random perturbation works by:

1. Generating a random angle θ between 0° and 360°
2. Generating a random distance *d* between 0 and a maximum threshold (e.g., 100 meters)
3. Displacing each point by distance *d* in direction θ

This creates a "cloud of uncertainty" around each true location. An attacker who obtains the masked coordinates cannot know if the true location is north, south, east, or west of the displayed point—only that it lies somewhere within the masking radius.

## Task 6.1: Apply Random Perturbation to Philadelphia Homicides

We will mask the homicide locations using a **500-meter threshold**. This value is chosen because:

- It exceeds the ~50m positional uncertainty already present from block-level geocoding
- It provides meaningful privacy protection (roughly one city block)
- It preserves neighborhood-level spatial patterns
- It is consistent with common geomasking practice in public health research

```{r geomasking-function}
# Function to apply random perturbation geomasking
# Inputs:
#   sf_object: an sf object with point geometries
#   max_distance_m: maximum displacement distance in meters
# Returns:
#   sf object with perturbed coordinates

geomask_random_perturbation <- function(sf_object, max_distance_m) {
  
  # Ensure we have point geometries
  if (!all(st_geometry_type(sf_object) == "POINT")) {
    stop("Input must contain only POINT geometries")
  }
  
  n_points <- nrow(sf_object)
  
  # Generate random angles (0 to 2*pi radians)
  random_angles <- runif(n_points, 0, 2 * pi)
  
  # Generate random distances (0 to max_distance_m)
  random_distances <- runif(n_points, 0, max_distance_m)
  
  # Calculate displacement in x and y (in meters)
  dx_m <- random_distances * cos(random_angles)
  dy_m <- random_distances * sin(random_angles)
  
  # Get current coordinates
  coords <- st_coordinates(sf_object)
  
  # Convert meter displacement to degrees (approximate)
  # At Philadelphia's latitude (~40°N):
  # 1 degree latitude ≈ 111,000 m
  # 1 degree longitude ≈ 85,000 m (111,000 * cos(40°))
  lat_center <- mean(coords[, 2])
  meters_per_deg_lat <- 111000
  meters_per_deg_lon <- 111000 * cos(lat_center * pi / 180)
  
 dx_deg <- dx_m / meters_per_deg_lon
  dy_deg <- dy_m / meters_per_deg_lat
  
  # Apply displacement
  new_coords <- coords
  new_coords[, 1] <- coords[, 1] + dx_deg
  new_coords[, 2] <- coords[, 2] + dy_deg
  
  # Create new sf object with masked coordinates
  masked_sf <- sf_object
  st_geometry(masked_sf) <- st_sfc(
    lapply(1:n_points, function(i) st_point(new_coords[i, ])),
    crs = st_crs(sf_object)
  )
  
  # Store the displacement distances for analysis
  masked_sf$mask_distance_m <- random_distances
  masked_sf$mask_angle_deg <- random_angles * 180 / pi
  
  return(masked_sf)
}
```

```{r apply-geomasking}
# Set seed for reproducibility (remove in production for true randomness)
set.seed(42)

# Apply 100-meter random perturbation
homicides_masked <- geomask_random_perturbation(homicides_sf, max_distance_m = 500)

# Summary of displacement distances
cat("=== GEOMASKING SUMMARY ===\n")
cat("Masking threshold: 100 meters\n")
cat("Points masked:", nrow(homicides_masked), "\n")
cat("\nDisplacement distances (meters):\n")
cat("  Min:", round(min(homicides_masked$mask_distance_m), 1), "\n")
cat("  Mean:", round(mean(homicides_masked$mask_distance_m), 1), "\n")
cat("  Max:", round(max(homicides_masked$mask_distance_m), 1), "\n")
```

```{r displacement-histogram, fig.width=8, fig.height=5}
# Visualize the distribution of displacement distances
ggplot(homicides_masked, aes(x = mask_distance_m)) +
  geom_histogram(bins = 20, fill = "steelblue", color = "white", alpha = 0.7) +
  geom_vline(xintercept = mean(homicides_masked$mask_distance_m), 
             color = "red", linetype = "dashed", size = 1) +
  annotate("text", x = mean(homicides_masked$mask_distance_m) + 5, y = Inf, 
           label = paste("Mean =", round(mean(homicides_masked$mask_distance_m), 1), "m"),
           vjust = 2, hjust = 0, color = "red") +
  labs(
    title = "Distribution of Random Perturbation Distances",
    subtitle = "100-meter maximum threshold",
    x = "Displacement Distance (meters)",
    y = "Count"
  ) +
  theme_minimal(base_size = 12)
```

**Question:** Why is the mean displacement approximately 67 meters rather than 50 meters (half of 100)?

**ANSWER:** This is because we are sampling uniformly across a circular area, not along a line. Points near the edge of the circle have more "area" to sample from than points near the center. The expected mean distance for uniform random sampling within a circle of radius *r* is (2/3)*r* ≈ 0.67*r*. With *r* = 100m, the expected mean is ~67m.

## Task 6.2: Compare Original and Masked Locations


```{r compare-maps, fig.width=12, fig.height=6}
# Create side-by-side comparison map

# Get coordinates for both datasets
original_coords <- st_coordinates(homicides_sf)
masked_coords <- st_coordinates(homicides_masked)

# Create comparison data frame
comparison_df <- data.frame(
  orig_lon = original_coords[, 1],
  orig_lat = original_coords[, 2],
  mask_lon = masked_coords[, 1],
  mask_lat = masked_coords[, 2],
  displacement = homicides_masked$mask_distance_m
)

# Create line geometries connecting original to masked locations
displacement_lines <- lapply(1:nrow(comparison_df), function(i) {
  st_linestring(matrix(
    c(comparison_df$orig_lon[i], comparison_df$orig_lat[i],
      comparison_df$mask_lon[i], comparison_df$mask_lat[i]),
    ncol = 2, byrow = TRUE
  ))
})

displacement_sf <- st_sf(
  displacement_m = comparison_df$displacement,
  geometry = st_sfc(displacement_lines, crs = 4326)
)

# Interactive comparison map with both layers
map_comparison <- leaflet() %>%
  addProviderTiles(providers$CartoDB.Positron) %>%
  
  # Displacement lines (gray, dashed)
  addPolylines(
    data = displacement_sf,
    color = "orange",
    weight = 2,
    opacity = 0.5,
    dashArray = "3,3",
    group = "Displacement Lines",
    popup = ~paste0("Displacement: ", round(displacement_m, 1), " m")
  ) %>%
  
  # Original locations (red)
  addCircleMarkers(
    data = homicides_sf,
    radius = 5,
    color = "red",
    fillColor = "red",
    fillOpacity = 0.6,
    weight = 1,
    group = "Original Locations",
    popup = ~paste0(
      "<strong>Original Location</strong><br>",
      "Date: ", dispatch_date, "<br>",
      "Block: ", location_block
    )
  ) %>%
  
  # Masked locations (blue)
  addCircleMarkers(
    data = homicides_masked,
    radius = 5,
    color = "blue",
    fillColor = "blue",
    fillOpacity = 0.6,
    weight = 1,
    group = "Masked Locations (100m)",
    popup = ~paste0(
      "<strong>Masked Location</strong><br>",
      "Displacement: ", round(mask_distance_m, 1), " m<br>",
      "Direction: ", round(mask_angle_deg, 0), "°"
    )
  ) %>%
  
  # Layer control
  addLayersControl(
    overlayGroups = c("Original Locations", "Masked Locations (100m)", "Displacement Lines"),
    options = layersControlOptions(collapsed = FALSE)
  ) %>%
  
  addLegend(
    position = "topright",
    colors = c("red", "blue", "gray"),
    labels = c("Original", "Masked (100m)", "Displacement"),
    title = "Location Type"
  ) %>%
  
  addScaleBar(position = "bottomleft")

map_comparison

# Save the comparison map
saveWidget(map_comparison, "philadelphia_homicides_masked_comparison.html", selfcontained = TRUE)
cat("\nComparison map saved to: philadelphia_homicides_masked_comparison.html\n")
```
```

**Question:** Toggle the layers on and off to compare the original and masked locations. At the city-wide scale, can you visually distinguish between them? What about when you zoom in to a single neighborhood?

**ANSWER:** At the city-wide scale, the overall spatial pattern (clustering in North and West Philadelphia) is preserved—the masked and original point clouds appear nearly identical. However, when you zoom in to a single neighborhood or block, individual points have clearly shifted. Some points that were on one side of a street now appear on the other side. This demonstrates the key principle of geomasking: **macro-level patterns are preserved while micro-level precision is sacrificed for privacy**.

## Task 6.3: Assess Impact on Spatial Analysis

Let's quantify how geomasking affects a common spatial analysis: counting incidents within police districts.

```{r district-comparison}
# Load Philadelphia police district boundaries (if available)
# For this exercise, we'll use the district codes already in the data
# and show how point-in-polygon assignments might change

# Count by district - original
district_original <- homicides_sf %>%
  st_drop_geometry() %>%
  count(dc_dist, name = "original_count")

# Count by district - masked (using original district assignments)
# Note: In practice, you would re-run point-in-polygon with masked coordinates
district_masked <- homicides_masked %>%
  st_drop_geometry() %>%
  count(dc_dist, name = "masked_count")

# Compare
district_compare <- district_original %>%
  left_join(district_masked, by = "dc_dist") %>%
  mutate(
    difference = masked_count - original_count,
    pct_change = round((difference / original_count) * 100, 1)
  )

kable(district_compare,
      caption = "District Counts: Original vs. Masked Locations",
      col.names = c("District", "Original", "Masked", "Difference", "% Change"))
```

**NOTE:** In this comparison, we used the original district assignments for both datasets. In a real scenario where you only have masked coordinates, you would need to perform point-in-polygon analysis with the masked points, which could result in some boundary-crossing misclassifications. With a 100m masking threshold, points within ~100m of a district boundary could be assigned to the wrong district.

## Task 6.4: Save Masked Dataset

```{r save-masked}
# Create a version suitable for public release
homicides_public <- homicides_masked %>%
  select(
    objectid,
    # Remove exact date - keep only month for temporal analysis
    month = dispatch_date,
    time_period,
    dc_dist,
    # Geometry is already masked
  ) %>%
  mutate(
    month = floor_date(month, "month")  # Round to first of month
  )

# Remove the masking metadata (don't reveal displacement distances!)
homicides_public$mask_distance_m <- NULL
homicides_public$mask_angle_deg <- NULL

# Save as GeoJSON for sharing
st_write(homicides_public, "philadelphia_homicides_masked_public.geojson", 
         delete_dsn = TRUE, quiet = TRUE)

cat("Public release dataset saved to: philadelphia_homicides_masked_public.geojson\n")
cat("Records:", nrow(homicides_public), "\n")
cat("Variables retained: objectid, month, time_period, dc_dist, geometry\n")
cat("\nPrivacy protections applied:\n")
cat("  ✓ 100m random spatial perturbation\n")
cat("  ✓ Dates aggregated to month\n")
cat("  ✓ Exact addresses removed\n")
```

## Reflection: Choosing a Masking Threshold

**Question:** We used 500 meters as our masking threshold. Discuss the trade-offs involved in choosing this value. Under what circumstances might you choose a larger threshold (e.g., 1000m)? A smaller one (e.g., 25m)?

**ANSWER:**

**Arguments for a larger threshold (e.g., 1000m):**
- Highly sensitive data (e.g., HIV cases, domestic violence)
- Rural areas where 500m might still identify a single property
- When only regional patterns matter, not neighborhood-level
- When re-identification risk is high (small population, rare event)
- When data will be publicly released without access controls

**Arguments for a smaller threshold (e.g., 25m):**
- When spatial precision is critical for the analysis (e.g., environmental exposure assessment)
- When data will remain within a secure research environment
- Urban areas with high population density (many potential "matches" within small radius)
- When other privacy protections are already in place (temporal aggregation, attribute suppression)
- When the original data already has substantial positional error (block-level geocoding)

**Key principle:** The masking threshold should be proportional to the privacy risk and inversely proportional to the analytical precision required. There is no universal "correct" value—it depends on context.

**NOTE:** Connect this to the Armstrong et al. (1999) paper in the references, which provides a comprehensive framework for evaluating geomasking methods and their trade-offs.

---

# Part 7: Your Own Dataset

Now that you have worked through the full workflow — loading, geocoding, assessing match rates, mapping, reflecting on accuracy and ethics, and applying geomasking — repeat the exercise on a dataset **you find yourself**.

## 7.1 Choose a Dataset

Find a publicly available dataset that contains **address-level location information** that you can geocode.  Good sources include:

- City open-data portals (e.g. Philadelphia OpenData, NYC Open Data, Chicago Data Portal)
- FBI Uniform Crime Reporting (UCR) downloads
- CDC / state health-department disease-surveillance files
- EPA enforcement or facility-location records
- Any other source that interests you and contains US addresses

**Requirements:**
- At least 50 geocodable addresses
- Addresses should be in a single metropolitan area (keeps the maps readable)
- The dataset should have at least one categorical or temporal variable you can use to colour or layer a map

**Question:** Describe your dataset.  What is the source?  How many records does it contain?  What variables are available?  Why did you choose it?

---

## 7.2 Geocode with the Census Bureau

Adapt the code from Part 2 to geocode your new dataset.

```{r your-geocode, eval=FALSE}
# --- TEMPLATE — adapt column names to your data ---
your_data_geocoded <- your_data %>%
  mutate(
    full_address = paste(ADDRESS_COLUMN, "CITY, STATE")
  ) %>%
  geocode(
    address = full_address,
    method  = "census",
    lat     = latitude,
    long    = longitude,
    verbose = TRUE
  )
```

**Question:** What is your match rate?  How does it compare to the Philadelphia homicide dataset?  If it differs, offer at least two plausible explanations (think about address format, geographic coverage of TIGER files, etc.).

---

## 7.3 Examine Your Failed Geocodes

```{r your-failures, eval=FALSE}
# --- TEMPLATE ---
your_failed <- your_data_geocoded %>%
  filter(is.na(latitude))

kable(head(your_failed, 10))
```

**Question:** What patterns do you see in the addresses that failed?  Are the failures random, or do they cluster by neighbourhood, address type, or some other feature?

---

## 7.4 Map Your Data

Create at least **two** maps:

1. A basic point map of all successfully geocoded records.
2. An enhanced map that uses colour or layer toggling to show variation by a categorical or temporal variable in your dataset.

```{r your-maps, eval=FALSE}
# --- TEMPLATE: basic map ---
library(sf)
library(leaflet)

your_sf <- your_data_geocoded %>%
  filter(!is.na(latitude)) %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326)

your_sf %>%
  leaflet() %>%
  addProviderTiles(providers$CartoDB.Positron) %>%
  addCircleMarkers(radius = 5, color = "steelblue", fillOpacity = 0.7)
```

**Question:** Describe the spatial pattern you observe.  Are the records clustered, or spread evenly?  Which areas have the highest and lowest concentrations?

---

## 7.5 Reflection

**Question:** Compare your experience geocoding and mapping this new dataset to the Philadelphia homicide exercise.  Address at least three of the following:

- How did your match rate compare, and why?
- Were the spatial patterns you observed surprising?  What might explain them?
- What ethical considerations apply to your dataset?  (Think about privacy, stigmatisation, potential for misuse.)
- If you were to publish a map of your data, what additional steps would you take before doing so?
- What limitations does positional uncertainty introduce for the specific analyses one might do with your dataset?


---

## 7.6 Apply Geographic Masking to Your Data

Now apply random perturbation geomasking to your geocoded dataset. Unlike the Philadelphia exercise where we specified a 500-meter threshold, **you must determine an appropriate threshold for your specific data**.

### Step 1: Choose Your Masking Threshold

Before writing any code, consider these factors:

1. **Sensitivity of the data**: How harmful would re-identification be?
   - Medical data, crime victimization, domestic violence → larger threshold
   - Restaurant inspections, business locations → smaller threshold (or none)

2. **Geographic context**: What does distance mean in your study area?
   - Rural area with dispersed properties → larger threshold (100m might identify a single farm)
   - Dense urban area → smaller threshold may suffice (many potential "matches" nearby)

3. **Analytical requirements**: What spatial precision do you need?
   - Neighborhood-level analysis → can tolerate 200-500m masking
   - Street-level or parcel-level analysis → masking may compromise results

4. **Existing positional uncertainty**: How accurate was your geocoding?
   - If geocoding already introduced ~100m error, adding 25m masking adds little privacy
   - Consider masking threshold relative to existing uncertainty

5. **Other privacy protections**: What else have you done?
   - If dates are aggregated and attributes removed, spatial masking can be smaller
   - If full attributes retained, spatial masking should be larger

**Question:** What masking threshold will you use for your dataset? Justify your choice by addressing at least three of the factors above.

**YOUR ANSWER:** [Write your threshold choice and justification here]

---

### Step 2: Apply Geomasking

Use the `geomask_random_perturbation()` function from Part 6.5 to mask your data.

```{r your-geomasking, eval=FALSE}
# --- TEMPLATE: Apply geomasking to your data ---

# First, ensure your data is an sf object
your_sf <- your_data_geocoded %>%
  filter(!is.na(latitude)) %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326)

# Set your chosen threshold (in meters)
my_threshold <- ___  # <-- Enter your threshold here

# Apply random perturbation
your_masked <- geomask_random_perturbation(your_sf, max_distance_m = my_threshold)

# Summary
cat("Masking threshold:", my_threshold, "meters\n")
cat("Points masked:", nrow(your_masked), "\n")
cat("Mean displacement:", round(mean(your_masked$mask_distance_m), 1), "m\n")
```

---

### Step 3: Compare Original and Masked Locations

Create a map comparing your original and masked locations, similar to Task 6.2.

```{r your-mask-comparison, eval=FALSE}
# --- TEMPLATE: Comparison map ---
leaflet() %>%
  addProviderTiles(providers$CartoDB.Positron) %>%
  addCircleMarkers(
    data = your_sf,
    radius = 5, color = "red", fillOpacity = 0.6,
    group = "Original"
  ) %>%
  addCircleMarkers(
    data = your_masked,
    radius = 5, color = "blue", fillOpacity = 0.6,
    group = paste0("Masked (", my_threshold, "m)")
  ) %>%
  addLayersControl(
    overlayGroups = c("Original", paste0("Masked (", my_threshold, "m)")),
    options = layersControlOptions(collapsed = FALSE)
  )
```

**Question:** At what zoom level do the original and masked points become visually distinguishable? Is this consistent with your chosen threshold?

---

### Step 4: Assess Impact on Your Analysis

Choose one spatial analysis relevant to your dataset (e.g., counts by administrative unit, distance to facilities, cluster detection) and assess how geomasking affects the results.

**Question:** Describe the analysis you performed and how geomasking changed the results. Was the impact acceptable given your analytical goals? If the impact was too large, would you consider reducing the threshold, or would you accept reduced precision to maintain privacy protection?

---

### Step 5: Reflection on the Privacy-Utility Trade-off

**Question:** Reflect on the overall geomasking process for your dataset. Address the following:

a) Did your chosen threshold provide adequate privacy protection for the sensitivity of your data? How do you know?

b) Did geomasking preserve the spatial patterns needed for your analytical purposes? What patterns were preserved, and what precision was lost?

c) If you were preparing this dataset for public release, would you use the same threshold? Why or why not?

d) What additional privacy protections (temporal aggregation, attribute suppression, access controls) might you combine with geomasking?

**NOTE:** There is no single "correct" threshold—the goal is to demonstrate thoughtful consideration of the privacy-utility trade-off in your specific context. Students who choose different thresholds for different types of data are demonstrating appropriate critical thinking.

---

# Summary and Deliverables



# References

Key papers relevant to this exercise:

1. **Jacquez, G.M. (2012).** "A research agenda: does geocoding positional error matter in health GIS studies?" *Spatial and Spatio-temporal Epidemiology*, 3(1), 7-16.
   - Mean positional errors: 58-614m depending on setting
   - Nearest neighbor misidentification increases with error and density

2. **Zimmerman, D.L. & Lahiri, S. (2008).** "On estimating the statistical power of disease cluster detection tests in case-control studies." *Statistics in Medicine*, 27(26), 5500-5520.
   - Geocoding failures are spatially clustered (p < 0.001)
   - Power loss of 50-70% when disease associated with geocoding failure

3. **Olson, K.L. et al. (2006).** "Effect of geocoding on accuracy of cluster detection in disease surveillance." *Advances in Disease Surveillance*, 1(1), 1-11.
   - Aggregating to census tract centroids reduces detection power 20-30%
   - Boundary crossing effect most severe

4. **Goldberg, D.W. (2008).** "A Geocoding Best Practices Guide.** University of Southern California GIS Research Laboratory.
   - Comprehensive guide to geocoding methodology
   - Best practices for accuracy assessment

5. **Armstrong, M.P. et al. (1999).** "Geographically masking health data to preserve confidentiality." *Statistics in Medicine*, 18, 497-525.
   - Methods for geographic privacy protection
   - Trade-offs between privacy and analytical utility

---

# Session Information
```{r session-info}
sessionInfo()
```
